services:
  api:
    profiles: ["api"]
    container_name: user_service.api
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    tty: true
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - ./config:/app/config:ro
    networks:
      - user_service.postgres.network
      - user_service.mongo.network
      - user_service.kafka.network
      - user_service.elastic
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-./config/prod_config.template.toml}
    depends_on:
      kafka:
        condition: service_healthy
    # healthcheck:
    #   test: ["CMD-SHELL", "curl -fsSL http://localhost:8000/healthcheck/"]
    #   interval: 10s
    #   timeout: 60s
    #   retries: 5
    #   start_period: 10s

  postgres_migration:
    profiles: ["migration"]
    container_name: user_service.postgres_migration
    build:
      context: .
    restart: on-failure
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - user_service.postgres.network
      - user_service.elastic
    volumes:
      - ./config:/app/config:ro
      - ./alembic.ini:/app/alembic.ini:ro
      - ./src/infrastructure/postgres/migrations:/app/src/infrastructure/postgres/migrations:ro
    environment:
      - CONFIG_PATH=${CONFIG_PATH:-./config/prod_config.toml}
    command: ["python", "-m", "alembic", "upgrade", "head"]

  postgres:
    profiles: ["postgres_db"]
    container_name: user_service.postgres
    image: "postgres:17rc1-alpine"
    hostname: user_service.postgres
    restart: unless-stopped
    ports:
      - "${POSTGRES_PORT}:5432"
    networks:
      - user_service.postgres.network
    environment:
      POSTGRES_PASSWORD: $POSTGRES_PASSWORD
      POSTGRES_USER: ${POSTGRES_USER:-$USER}
      POSTGRES_DB: ${POSTGRES_DB:-$USER}
    volumes:
      - user_service.postgres.data:/var/lib/postgresql/data:rw
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 10s
      timeout: 60s
      retries: 5
      start_period: 10s

  postgres_backup:
    profiles: ["postgres_backup"]
    container_name: user_service.postgres_backup
    image: prodrigestivill/postgres-backup-local:15-alpine
    networks:
      - user_service.postgres.network
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_DB=${POSTGRES_DB:-$USER}
      - POSTGRES_USER=${POSTGRES_USER:-$USER}
      - POSTGRES_PASSWORD=$POSTGRES_PASSWORD
      - BACKUP_DIR=/backups
      - POSTGRES_EXTRA_OPTS=-Z6 --schema=public --blobs
      - SCHEDULE=${POSTGRES_BACKUP_SCHEDULE:-@daily}
      - HEALTHCHECK_PORT=8080
    volumes:
      - ${POSTGRES_BACKUP_DIR:-./.backups/postgres}:/backups

  pgadmin:
    profiles: ["postgres_db"]
    container_name: user_service.pgadmin4
    image: dpage/pgadmin4
    restart: unless-stopped
    networks:
      - user_service.postgres.network
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"

  mongodb:
    profiles: ["mongo_db"]
    container_name: user_service.mongodb
    image: mongo
    ports:
      - "27017:27017"
    volumes:
      - user_service.mongodb.data:/data/db
    networks:
      - user_service.mongo.network
      - user_service.elastic

  mongo-express:
    profiles: ["mongo_db"]
    container_name: user_service.mongo-express
    image: mongo-express
    restart: unless-stopped
    ports:
      - "28081:8081"
    networks:
      - user_service.mongo.network
      - user_service.elastic
    environment:
      ME_CONFIG_MONGODB_SERVER: mongodb
      ME_CONFIG_BASICAUTH_USERNAME: ${MONGO_DB_ADMIN_USERNAME}
      ME_CONFIG_BASICAUTH_PASSWORD: ${MONGO_DB_ADMIN_PASSWORD}
      ME_CONFIG_MONGODB_URL: ${MONGO_DB_CONNECTION_URI}
    depends_on:
      - mongodb

  zookeeper:
    profiles: ["kafka"]
    image: confluentinc/cp-zookeeper:latest
    container_name: main-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 22181:2181
    networks:
      - user_service.kafka.network

  kafka:
    profiles: ["kafka"]
    image: confluentinc/cp-kafka:latest
    container_name: main-kafka
    depends_on:
      - zookeeper
    ports:
      - 29092:29092
      - 9092:9092
    hostname: kafka
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - user_service.kafka.network
    healthcheck:
      test: nc -z localhost 29092
      # interval: 10s
      # timeout: 10s
      # retries: 10
    # healthcheck:
    #   test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka:29092"]
    #   interval: 10s
    #   timeout: 10s
    #   retries: 10

  kafka-ui:
    profiles: ["kafka"]
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    ports:
      - 8090:8080
    restart: unless-stopped
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
    links:
      - kafka
      - zookeeper
    networks:
      - user_service.kafka.network

  # create-topics:
  #   image: confluentinc/cp-kafka:latest
  #   container_name: create-topics
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   entrypoint:
  #     - /bin/sh
  #     - -c
  #     - |
  #       echo "Waiting for Kafka to be ready..."
  #       cub kafka-ready -b kafka:29092 1 20 && \
  #       kafka-topics --create --topic Chat --partitions 3 --replication-factor 1 --if-not-exists --bootstrap-server kafka:29092 && \
  #       kafka-topics --create --topic Message --partitions 2 --replication-factor 1 --if-not-exists --bootstrap-server kafka:29092 && \
  #       echo "Topics created!"
  #   networks:
  #     - user_service.kafka.network
  #   restart: "no"

  # create-topics:
  #   image: confluentinc/cp-kafka:latest
  #   container_name: create-topics
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   volumes:
  #     - ./entrypoint.sh:/usr/local/bin/entrypoint.sh
  #   entrypoint:
  #     - /bin/sh
  #     - /usr/local/bin/entrypoint.sh
  #   networks:
  #     - user_service.kafka.network
  #   restart: "no"

  apm-server:
    profiles: ["elk"]
    container_name: user_service.apm-server
    image: elastic/apm-server:7.17.22
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
      - 8200:8200
    networks:
      - user_service.elastic
    command: >
      apm-server -e
        -E apm-server.rum.enabled=true
        -E setup.kibana.host=kibana:5601
        -E setup.template.settings.index.number_of_replicas=0
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=kibana:5601
        -E output.elasticsearch.hosts=["elasticsearch:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  elasticsearch:
    profiles: ["elk"]
    container_name: user_service.elasticsearch
    image: elasticsearch:8.14.1
    environment:
      - ELASTIC_PASSWORD=pass
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - discovery.type=single-node
      - xpack.security.enrollment.enabled=true
      # - xpack.security.enrollment.enabled=false
      - xpack.security.enabled=false
    # environment:
    #   - bootstrap.memory_lock=true
    #   - cluster.name=docker-cluster
    #   - cluster.routing.allocation.disk.threshold_enabled=false
    #   - discovery.type=single-node
    #   - ES_JAVA_OPTS=-XX:UseAVX=2 -Xms1g -Xmx1g
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
      - user_service.esdata:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - user_service.elastic
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  kibana:
    profiles: ["elk"]
    container_name: user_service.kibana
    image: kibana:8.14.1
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - 5601:5601
    networks:
      - user_service.elastic
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

volumes:
  user_service.postgres.data:
  user_service.mongodb.data:
  user_service.esdata:

networks:
  user_service.postgres.network:
  user_service.mongo.network:
  user_service.kafka.network:
  user_service.elastic:
